---
layout: distill
title: Safe RL
description: "A Multiplicative Value Function for Safe and Efficient Reinforcement Learning"
date: 2023-08-22
permalink: /saferl

authors:
  - name: Nick Bührer
    url: "https://www.linkedin.com/in/nick-b%C3%BChrer-250b46174/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Zhejun Zhang
    url: "https://zhejz.github.io/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Alex Liniger
    url: "https://alexliniger.github.io/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Fisher Yu
    affiliations:
      name: CVL, ETH Zürich
  - name: Luc Van Gool
    url: "https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjcxLC0xOTcxNDY1MTc4.html"
    affiliations:
      name: CVL, ETH Zürich <br> PSI, KU Leuven <br> INSAIT, Sofia

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

toc:
  - name: Abstract
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Overview
  - name: Supplementary Videos
  - name: Citation
  # - name: Layouts
  # - name: Other Typography?

---
<center>
<div class="links">
<a href="https://arxiv.org/abs/2303.04118" class="btn btn-sm z-depth-1" role="button" target="_blank">arXiv</a>
<a href="https://github.com/nikeke19/Safe-Mult-RL" class="btn btn-sm z-depth-1" role="button" target="_blank">Code</a>
<a href="/assets/pdf/saferl_arxiv.pdf" class="btn btn-sm z-depth-1" role="button" target="_blank">PDF</a>
<a href="/assets/pdf/saferl_poster.pdf" class="btn btn-sm z-depth-1" role="button" target="_blank">Poster</a>
<a href="/assets/pdf/saferl_slides.pdf" class="btn btn-sm z-depth-1" role="button" target="_blank">Slides</a>
</div>
</center>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/saferl-banner.png" title="saferl banner" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

## Abstract

An emerging field of sequential decision problems is safe Reinforcement Learning (RL), where the objective is to maximize the reward while obeying safety constraints. Being able to handle constraints is essential for deploying RL agents in real-world environments, where constraint violations can harm the agent and the environment. To this end, we propose a safe model-free RL algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. The safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. By splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. We integrate our approach into two popular RL algorithms, Proximal Policy Optimization and Soft Actor-Critic, and evaluate our method in four safety-focused environments, including classical RL benchmarks augmented with safety constraints and robot navigation tasks with images and raw Lidar scans as observations. Finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. Our code can be found at https://github.com/nikeke19/Safe-Mult-RL.

## Overview

<iframe width="720" height="405" src="https://www.youtube.com/embed/5yr__lEK_bU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Supplementary Videos

<iframe width="720" height="405" src="https://www.youtube.com/embed/mw05UC4Imfc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Citation

{% highlight bibtex %}
@inproceedings{buehrer2023multiplicative,
  title = {A Multiplicative Value Function for Safe and Efficient Reinforcement Learning},
  booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
  author = {Bührer, Nick and Zhang, Zhejun and Liniger, Alexander and Yu, Fisher and Van Gool, Luc},
  year = {2023},
}
{% endhighlight %}
