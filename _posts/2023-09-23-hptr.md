---
layout: distill
title: HPTR
description: "Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding"
date: 2023-09-23
permalink: /hptr

authors:
  - name: Zhejun Zhang
    url: "https://zhejz.github.io/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Alex Liniger
    url: "https://alexliniger.github.io/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Christos Sakaridis
    url: "http://people.ee.ethz.ch/~csakarid/"
    affiliations:
      name: CVL, ETH Zürich
  - name: Fisher Yu
    affiliations:
      name: CVL, ETH Zürich
  - name: Luc Van Gool
    url: "https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjcxLC0xOTcxNDY1MTc4.html"
    affiliations:
      name: CVL, ETH Zürich <br> PSI, KU Leuven <br> INSAIT, Sofia

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

toc:
  - name: Abstract
  - name: Efficiency and Scalability
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Overview
  # - name: Supplementary Videos
  - name: Citation
  # - name: Layouts
  # - name: Other Typography?

---
<center>
<div class="links">
<a href="https://arxiv.org/abs/2310.12970" class="btn btn-sm z-depth-1" role="button" target="_blank">arXiv</a>
<a href="https://github.com/zhejz/HPTR" class="btn btn-sm z-depth-1" role="button" target="_blank">Code</a>
<a href="/assets/pdf/hptr_poster.pdf" class="btn btn-sm z-depth-1" role="button" target="_blank">Poster</a>
<a href="/assets/pdf/hptr_slides.pdf" class="btn btn-sm z-depth-1" role="button" target="_blank">Slides</a>
</div>
</center>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/hptr-banner.png" title="hptr banner" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

## Abstract

The real-world deployment of an autonomous driving system requires its components to run on-board and in real-time, including the motion prediction module that predicts the future trajectories of surrounding traffic participants. Existing agent-centric methods have demonstrated outstanding performance on public benchmarks. However, they suffer from high computational overhead and poor scalability as the number of agents to be predicted increases. To address this problem, we introduce the K-nearest neighbor attention with relative pose encoding (KNARPE), a novel attention mechanism allowing the pairwise-relative representation to be used by Transformers. Then, based on KNARPE we present the heterogeneous polyline Transformer with relative pose encoding (HPTR), a hierarchical framework enabling asynchronous token update during the online inference. By sharing contexts among agents and reusing the unchanged contexts, our approach is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods. Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior performance among end-to-end methods that do not apply expensive post-processing or ensembling. The code is available at [https://github.com/zhejz/HPTR](https://github.com/zhejz/HPTR).

## Efficiency and Scalability

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/hptr-efficiency.png" title="hptr efficiency" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
HPTR is as efficient as scene-centric (SC) methods in terms of GPU memory consumption and inference latency, while being as accurate as agent-centric methods. We use standard Ubuntu, Python and Pytorch without optimizing for real-time deployment. We predict one scenario at each inference time on one 2080Ti. We repeat the inference of the same scenario for 100 times to simulate online inference, where the static map features are computed at the first step and reused for the next 99 steps. Compared to SOTA agent-centric method Wayformer (WF baseline), we achieve similar performance while reducing the offline inference latency by 60%, and the GPU memory consumption as well as the inference latency by 80%. By caching the static map features during online inference, HPTR can generate predictions for 64 agents in real time at 40 frames per second.

## Overview

[5-min Presentation Video on SlidesLive](https://recorder-v3.slideslive.com/?share=85437&s=4af2f2e3-d823-4476-8239-d5d48c110b04)


## Citation

{% highlight bibtex %}
@inproceedings{zhang2023hptr,
  title = {Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  author = {Zhang, Zhejun and Liniger, Alexander and Sakaridis, Christos and Yu, Fisher and Van Gool, Luc},
  year = {2023},
}
{% endhighlight %}
